IMPORTANT STARTING NOTE: source activate pytorch
- For Training: 
    python3 src/main.py --config=vdn --env-config=marl_partial
    - config: customize.yaml (训练时关闭debug)
    (load existing pointer : checkpoint_path at customize.yaml)
- For Testing: 
    python3 src/test.py --config=vdn --env-config=marl_partial
    (marl_partial.yaml: debug: True)
    (test.yaml: checkpoint_path: ..., evaluate: True)
- For Visualization: (in another terminal)   
    cd src/vis_export/
    python3 src/visualizer_server.py
    





TESTING & EVALUATION
    - yaml file 里面规定 [evaluate: True] 即可 Evaluate model for test_nepisode episodes (具体由env.yaml决定) and quit (no training).
用 [python3 src/test.py --config=vdn --env-config=marl_partial] 进行测试. 会进入 test_run.py - run - run_sequential - checkpoint_path
(因此需要提供evaluate的checkpoint_path) - evaluate_sequential - runner.run(test_mode=True) - return - exit. 注意 runner.run 这里会
call episode_runner.run(self, test_mode) 并且return一个buffer记录历史. 

- test 的时候可以把 marl_partial.yaml 里面的 debug: False 弄成 True，可以在跑env.step()的时候看见一些信息
TODO: 最好把这些弄进一个file里面看会方便一些。

- 原始版本
    - 目前跑 empty-8-8 map 10 agent 的结果：大概 9000000 (8hr) 次之后，test_return_mean 从-200提升到了-12左右 
    (这里已经跑过了上面说的test，和结果吻合，avg eps rew在-13左右)，但是ep_len_mean一直是100没有变过 => never finished。
    - 可能的原因：1) ending criteria 有问题导致 agents 完成目标也无法结束任务，2) agents经过这么久的训练仍不能在100ep之内完成所有目标。
    3) 100的 limit 很难by chance 让所有 agent 都到达各自的 goal，永远没有拿到最大的sparse reward，因此没有motivation去“完成”
    - TODO: idea: 1. inverse goal distance reward (pre-computed shortest path distance). 
    idea: 2/1. for each agent 到goal给，走goal扣 2/2. 只要在goal就给一个小reward。
    - TODO: 如果有了pre-computed shortest path distance，是否也可以用作obs?

- 第一改版：根据 inverse goal (pre-computed shortest path) distance 每一步给出extra reward
    - 目前跑 empty-8-8 map 10 agent 的结果：大概 1820100 (2hr) 次之后，test_return_mean 从-200提升到了-15左右 
    ep_len_mean仍然是是100没有变过 =》没有学到需要去终点。可能需要加其他的奖励。
    - 可能的改进：


- 对 episode_runner self.batch 的理解: ( 即test_run的test_buffer = runner.run(...) )
    self.batch[key][0] 是 length=episode_len 个 datapoint，每个对应一个step
    self.batch[key][0][k] 是 第k-step 时key的信息，length=n_agents
    self.batch[key][0][k][i] 可以access到某个step某个agent的具体信息，可通过这个来可视化

- recall每个action的意思: 0 left (x-1,y), 1 right (x+1,y), 2 up (x,y-1), 3 down (x,y+1), 4 stay still

- recall: graph坐标：upper-left corner=(0,0), _grid_shape=(#cols, #rows)
    access self._original_grid / _full_obs by [row id][col id],
    也就是说 用 G[y][x] 来 access 一个specific grid
    G[y] 是某个 row

- 目前没办法可视化(该怎么显示?)

3/3 - 1:    reward: closer & staygoal=1 & collision=-10 & completion=1000 
            converge to not moving
3/3 - 2:    reward: closer & stay=-0.1 & staygoal=1 & collision=-10 & completion=1000 
            converge to not moving
3/3 - 3:    reward: closer & stay=-0.1 & staygoal=1 & collision=-1 & completion=1000 
            starting learning high rewards
3/6 - 1:    reward: closer & stay=-0.1 & staygoal=1 & collision=-1 & completion=1000+#agent(hor-#t)*fac, fac=1.5
            rew = -3630     len = 100   before  batch = 0
            rew = 30739746  len = 10.5  after   batch = 1800000
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-03-08_20-02-46"
            
            new reward: ... collision=-5 ...    
            rew = 37670059  len = 17.1  after   batch = 2200000
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-03-08_23-47-03"

            new reward: ... collision=-20 ...  
            rew = 25880127  len = 35.6  after   batch = 2700394             collision
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-03-09_06-22-13"

            new reward: ... collision=-100 ...  
            start rew = 414877      start len = 32      batch = 2700394
            after rew = 23671932    after len = 40.6    batch = 3900000     collision
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-03-09_18-36-54"

            new reward: ... collision=-1000 ...  
            start rew = -5100       start len = 42      batch = 3900000
            after rew = 25763181    after len = 43      batch = 4500000     collision
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-03-09_21-01-54"


Total Cost = 每个agent到终点t之和（到之后又离开算什么？）- 最后一次呆在goal不动的时间点
Make Span = (eps len)最慢agent到终点t
Collision Number = total collision number
* Visulize *
Testing Break Tie
    - Q-value (evaluate.py) 2max action => 2max 不确定是否会撞，目前撞不撞是由env detect (marlenv.py)
    - env (marlenv.py) detect到collision后做一些处理，但是这样就无法access到Q-value来选择suboptimal action (evaluate.py)
    - write new evaluate.py 




TODO
    1. visualize 加 timestep
    2. total cost definition
    3. discount 影响 reward？如果在t完成，+= complete_reward * 1 / (gamma ^ (hor - t)), 重新 train

==================================================================================================================
NEW TRAINING with updated complete:
4-18 -  1:  reward: closer & stay=-0.1 & staygoal=1 & collision=-1 & completion=1000 ...
            tr_rew:-410     te_rew:-3230    tr_len:100  te_len:100  t_env:0(start)
            tr_rew:53700    te_rew:54500    tr_len:15   te_len:13   t_env:1601030(end)
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-18_23-04-35"
        2:  ...collision=-5...
            tr_rew:55550    te_rew:55836    tr_len:10   te_len:10   t_env:1600379(start)
            tr_rew:56079    te_rew:55500    tr_len:9    te_len:10   t_env:2300419(end)       
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-19_00-34-46"
        3:  ...collision=-500...
            tr_rew:41048    te_rew:46312    tr_len:13   te_len:10   t_env:2300423(start)
            tr_rew:47974    te_rew:50044    tr_len:13   te_len:12   t_env:3280678(end)
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-19_04-49-13"
        X4.  ...collision=-5000...
            tr_rew:-46480   te_rew:7629     tr_len:14   te_len:12   t_env:3200518(start)
            explode and converge to len=100 ...
        X4.  ...collision=-2000..., 
            tr_rew:31450    te_rew:36777    tr_len:18   te_len:12   t_env:3200522(start)
            explode and converge to len=100 ...
        X4.  ...collision=-1000..., lr: 0.0005->0.0002 (slower learner) 
            tr_rew:44034    te_rew:45468    tr_len:13   te_len:11   t_env:3200517(start)
            explode and converge to len=100 ...
        X4.  ...collision=-800... 
            tr_rew:48182    te_rew:47020    tr_len:12   te_len:11   t_env:3200516(start)
            explode and converge to len=100 ...
        4.  ...collision=-500...
            tr_rew:50651    te_rew:49850    tr_len:10   te_len:12   t_env:3200514(start)
            tr_rew:49365    te_rew:51464    tr_len:12   te_len:11   t_env:4120823
            tr_rew:50131    te_rew:52199    tr_len:13   te_len:11   t_env:4761034
            tr_rew:50565    te_rew:52697    tr_len:13   te_len:11   t_env:5501331(end)  avg_collide = 2.18
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-19_17-00-24"
        5.  ...collision=-1000...
            tr_rew:37987    te_rew:51035    tr_len:13   te_len:11   t_env:5500670(start)
            tr_rew:48265    te_rew:51587    tr_len:14   te_len:13   t_env:6380976       avg_collide = 1.31
            tr_rew:48273    te_rew:51822    tr_len:14   te_len:12   t_env:6601074(end)  avg_collide = 1.28
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-20_03-13-33"
        X6. ...collision=-2000...
            tr_rew:41995    te_rew:48983    tr_len:13   te_len:13   t_env:6600775(start)
            explode and converge to len=100 ...
        6.  ...collision=-1000...
            tr_rew:52532    te_rew:51712    tr_len:12   te_len:13   t_env:6600774(start)
            tr_rew:48597    te_rew:51672    tr_len:14   te_len:13   t_env:7200949       avg_collide = 1.04
            tr_rew:49051    te_rew:52293    tr_len:14   te_len:13   t_env:9061548       avg_collide = 0.93
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-20_14-08-56"
        7.  ...collision=-2000...
            tr_rew:50515    te_rew:50912    tr_len:12   te_len:13   t_env:9000971(start)
            tr_rew:49905    te_rew:50552    tr_len:21   te_len:15   t_env:9601046       avg_collide = 0.71
            "/home/ubuntu/DongmingShen/MARL_curve/results/models/vdn__2023-04-21_00-09-39"
        8.  ...collision=-2000...
            tr_rew:49905    te_rew:50552    tr_len:21   te_len:15   t_env:9601046       avg_collide = 0.71
            tr_rew:44762    te_rew:50165    tr_len:17   te_len:15   t_env:10281317      avg_collide = 0.68
            tr_rew:44988    te_rew:50627    tr_len:17   te_len:15   t_env:11161656      avg_collide = 0.58

TODO:
1. env break tie at runtime to avoid collision (add a param to specify run time)
2. total cost : stop moving at goal
3. yes/no for collision, not count ?

4. lower bound?

curriculum learning